[
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "Research",
    "section": "",
    "text": "Research Question 1:\nDo certain demographic groups have a higher or lower likelihood of defaulting on credit card payments?\n\nMethodology:\nRegression Model: This methodology involves using logistic regression to analyze the likelihood of defaulting on credit card payments based on demographic variables.\nData Preprocessing:\nCategorical variables such as Gender, Education, Marriage, and default_payment_next_month are converted to factors to ensure compatibility with the regression model.\nExploratory Data Analysis (EDA):\nVisualizations are created to explore default payments across different demographic groups. This step helps to understand any patterns or trends in default behavior based on demographic characteristics.\nData exploration is performed using visualization to understand the distribution of default payments across different demographic groups. This is achieved through the creation of a bar plot (‘plot1’) using ‘ggplot2’ library.\n\n\n\n\n\nModel Training:\n\nThe dataset is split into training and testing sets using the ‘createDataPartition()’ function from the ‘caret’ library. This ensures that the model is trained on a subset of the data and evaluated on unseen data.\nA logistic regression model (‘logit_model’) is trained using the ‘glm()’ function. Logistic regression is a common method for binary classification tasks like predicting default payments.\n\nModel Evaluation:\n\n\nConfusion Matrix and Statistics\n\n                 \npredicted_classes Default Not Default\n      Default        1508        6795\n      Not Default     482         214\n                                          \n               Accuracy : 0.1914          \n                 95% CI : (0.1833, 0.1996)\n    No Information Rate : 0.7789          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : -0.0991         \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.75779         \n            Specificity : 0.03053         \n         Pos Pred Value : 0.18162         \n         Neg Pred Value : 0.30747         \n             Prevalence : 0.22114         \n         Detection Rate : 0.16757         \n   Detection Prevalence : 0.92266         \n      Balanced Accuracy : 0.39416         \n                                          \n       'Positive' Class : Default         \n                                          \n\n\n\nPredictions are made on the test set using the trained logistic regression model.\nPredicted classes are determined based on the probability thresholds (in this case, 0.5) using an ifelse statement.\nThe confusion matrix is generated using the ‘confusionMatrix()’ function from the caret library. This matrix provides insights into the performance of the model, including metrics such as accuracy, sensitivity, specificity, etc.\n\n\n\nInterpretation:\nThe graph visually dissects default payment statuses across gender, education, and marital status combinations, offering valuable insights into default likelihoods across different demographic groups. It reveals trends suggesting that males may exhibit slightly higher default rates in certain contexts. Higher education levels, particularly advanced degrees, are associated with lower default rates, while lower education levels show higher default frequencies, especially among singles. Marital status nuances also influence default propensity, with married individuals, especially those with advanced degrees, displaying lower default rates. Certain combinations, like high school education and single status, appear more vulnerable to defaults. Overall, the graph provides detailed statistical representations enabling nuanced analysis of default trends within micro-demographics.\nIn conclusion, certain demographic groups indeed have a higher or lower likelihood of defaulting. Males in several panels, individuals with lower educational attainment, and singles are more prone to defaulting. Conversely, those with advanced degrees, particularly when married, show a lower likelihood of default. These insights are critical for credit risk management, allowing for tailored risk assessment strategies and targeted financial advice or product offerings to mitigate potential defaults.\n\n\n\nResearch Question 2:\nWhich features are deemed most important in predicting credit default risk?\n\nMethodology:\nRandom Forest Model: This methodology involves training a Random Forest model, extracting variable importance measures, sorting them, and then visualizing the feature importance to gain insights into which features are most important in predicting credit default risk.\nSetting Seed for Reproducibility:\nSetting a seed (set.seed(1234)) ensures that the random number generation during model training is reproducible. This means that if you rerun the code with the same seed, you should get the same results.\nTraining the Random Forest Model:\nThe randomForest function is used to train a Random Forest model. This function requires specifying the formula for the model, where default_payment_next_month ~ . indicates that default_payment_next_month is the target variable, and . indicates that all other variables in the dataset are used as predictors.\nVariable Importance Measures: The importance function is applied to the trained Random Forest model (rf_model) to obtain variable importance measures. Variable importance provides insights into which predictor variables are most influential in predicting the target variable (credit default risk). The measure used here is the Mean Decrease in Gini Index. The obtained variable importance measures are sorted in descending order using the order function. Sorting helps in identifying the most important predictor variables at the top of the list.\n\n\n  Repay_Sept           ID Billpay_Sept          Age  Billpay_Aug Amtpaid_Sept \n  1018.32726    672.52568    563.75433    534.11658    512.46432    493.17382 \n Billpay_Jul  Limited_Bal  Billpay_Jun  Billpay_Apr  Billpay_May  Amtpaid_Aug \n   491.20440    490.99607    478.71140    476.84633    469.35011    455.75366 \n Amtpaid_Jul  Amtpaid_Apr  Amtpaid_May  Amtpaid_Jun    Repay_Aug    Repay_Jul \n   436.19727    433.73568    417.53178    413.28457    410.82039    291.82828 \n   Repay_Jun    Repay_May    Repay_Apr    Education     Marriage       Gender \n   234.61829    214.68302    210.96277    184.38949    109.52670     88.19018 \n\n\nThe sorted variable importance measures are printed to the console like above, allowing you to see the importance values of each predictor variable.\n\nFeature Importance:\n- After obtaining the variable importance measures, the code further processes them to extract the importance values and corresponding feature names.\n- The importance values are sorted in descending order, and the corresponding feature names are ordered accordingly.\n- This plot provides a graphical representation of each feature’s importance in predicting credit default risk, making it easier to interpret and identify the most influential features.\nConclusion: The insights gained from this analysis can inform credit risk assessment processes and aid in the development of more accurate predictive models for credit default risk.\n\n\nInterpretation:\nThe bar chart illustrates the importance of various features in predicting credit default risk, with repayment status in September (Repay_Sept) emerging as the most critical predictor. This highlights the significance of recent financial behaviors in forecasting default likelihood. The model also emphasizes the importance of bill amounts and repayment statuses in other months, underscoring the value of consistent financial habits over time. Conversely, demographic features like Gender, Education, and Marriage have lesser impact, suggesting that behavioral factors outweigh demographic variables in predicting default risk. These insights enable financial institutions to refine their risk assessment models, tailor monitoring strategies, and develop targeted interventions for customers exhibiting risky financial behaviors.\nIn summary, the Random Forest model identifies repayment statuses, particularly the most recent ones, and bill amounts as the most informative features for predicting credit default risk. This aligns with the notion that recent and consistent financial behaviors provide a clearer picture of a customer’s financial health than static demographic data. This insight is invaluable for developing more effective risk assessment tools and customer management strategies in the financial sector.\n\n\n\nResearch Question 3:\nIdentify the distinct patterns of credit limit utilization among users.\n\nMethodology:\nClustering: The methodology to answer the above question involves several key steps for clustering users based on their patterns of credit limit utilization.\nData Selection:\nRelevant columns related to credit limit utilization are chosen from the dataset. These columns typically include variables representing the repayment behavior across different months.\nData Standardization:\nThe selected data is standardized to ensure that all variables have a mean of 0 and a standard deviation of 1. This process, known as z-score normalization, is essential for clustering algorithms like K-means, as it prevents variables with larger scales from dominating the analysis.\nDetermining Optimal Number of Clusters:\nThe code employs the “elbow method” to determine the optimal number of clusters. This method involves calculating the within-cluster sum of squares for different numbers of clusters and plotting these values. The point where adding more clusters does not significantly decrease the within-cluster sum of squares indicates the optimal number of clusters.\nPerforming K-means Clustering:\nOnce the optimal number of clusters is determined, the K-means algorithm is applied to the standardized data. K-means is an iterative algorithm that partitions the data into the specified number of clusters by minimizing the within-cluster variance. Assigning Cluster Labels: Each user is assigned a cluster label based on the results of the K-means clustering algorithm. These labels represent the cluster to which each user belongs.\nAdding Cluster Labels to the Dataset:\nThe cluster labels are added back to the original dataset, allowing for further analysis and interpretation of the clustering results.\nVisualizing Cluster Centers:\nThe code performs Principal Component Analysis (PCA) on the cluster centers to reduce the dimensionality of the data. PCA transforms the high-dimensional cluster centers into a lower-dimensional space while preserving the most important information. The reduced-dimensional data is then visualized to gain insights into the distinct patterns of credit limit utilization among different clusters.\nOverall, this methodology enables the identification of meaningful patterns in credit limit utilization behavior and provides insights into how users can be grouped based on these patterns.\n\n\nInterpretation:\nThe images and corresponding data processing steps describe the utilization of K-means clustering to identify distinct patterns of credit limit utilization among users based on multiple credit-related behaviors.\nElbow Plot:\nThe elbow plot is used to determine the optimal number of clusters by showing the within-groups sum of squares (WSS) for different numbers of clusters. This metric measures the compactness of the clusters, with lower values generally indicating better clustering (less spread within the clusters).\n\nKey Observations from the Elbow Plot:\nA sharp decline in WSS from 2 to 3 clusters suggests significant improvements in cluster compactness with the addition of clusters in this range. The curve begins to flatten around 3 clusters, suggesting diminishing returns on improvement in cluster compactness with additional clusters. This is a typical indicator used to select the optimal number of clusters; in this case, it suggests that 3 clusters may be suitable. Second Image: PCA Plot of Clusters The second image displays a scatter plot of the first two principal components of the standardized data, colored by the assigned cluster labels (1, 2, 3). This visualization helps in interpreting the spatial distribution of clusters in the reduced dimensionality space.\nPCA Plot of Clusters:\nThe below graph displays a scatter plot of the first two principal components of the standardized data, colored by the assigned cluster labels (1, 2, 3). This visualization helps in interpreting the spatial distribution of clusters in the reduced dimensionality space.\n\nKey Observations from PCA Plot:\nCluster 1 (Pink): This cluster is distinctly positioned towards the positive side of the second principal component. This might indicate a specific pattern of credit utilization or repayment behavior that separates this group from the others.\nCluster 2 (Green): Occupying the middle ground in the plot, this cluster shows moderate values on both principal components. This could suggest an average credit behavior compared to the extremes exhibited by the other two clusters.\nCluster 3 (Black): Concentrated primarily in the center and negative side of the first principal component, this cluster could represent users with lower credit utilization or more consistent repayment behaviors compared to the others. Identifying Distinct Patterns of Credit Limit Utilization Based on the clustering analysis, three distinct patterns of credit utilization among users can be identified:\nHigh Utilization/Erratic Repayment: Likely represented by Cluster 1, these users might be utilizing their credit limits to a high extent and might have more volatile repayment histories. Moderate Utilization/Stable Repayment: Cluster 2’s positioning suggests users in this group have moderate credit utilization and more stable repayment patterns. Low Utilization/Consistent Repayment: Cluster 3 could be indicative of users who use their credit limits conservatively and maintain consistent repayment behaviors.\nConclusion:\nThese clusters help in understanding the different strategies and financial behaviors of credit card users, which can be crucial for credit risk management and marketing strategies tailored to different consumer segments. By recognizing these distinct patterns, financial institutions can better cater to the varying needs and risks associated with different groups of users."
  },
  {
    "objectID": "Ref.html",
    "href": "Ref.html",
    "title": "References",
    "section": "",
    "text": "Graph 1 Reference:\nRaj, A. (2023b, May 28). 12 Bad data visualization examples explained - Code conquest. Code Conquest.\nGraph 2 Reference:\nClayton, T., & Clayton, T. (2021b, June 22). 15 misleading data visualization examples. Rigorous Themes.\nOther References (for code and implementation):\n\nDr. Isuru Dassanayake week 4\nDr. Isuru Dassanayake week 6\nQuarto - about pages. (n.d.). Quarto."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dataset",
    "section": "",
    "text": "The dataset named ‘Default of Credit Card Clients’ comprises information pertaining to credit card holders in 2005 at Taiwan, encompassing various demographic and financial attributes. Each entry is uniquely identified by an ID. The dataset includes the credit limit (LIMIT_BAL) assigned to each individual, their gender (SEX), level of education (EDUCATION), marital status (MARRIAGE), and age. Additionally, it contains details on the repayment status (PAY_0 to PAY_6) over the past six months and it’s scale: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; …; 8 = payment delay for eight months; 9 = payment delay for nine months and above., bill amounts (BILL_AMT1 to BILL_AMT6), and payment amounts (PAY_AMT1 to PAY_AMT6) during the same period. The final column indicates whether the individual defaulted on their payment in the following month (default payment next month). This dataset holds potential for comprehensive analyses to discern patterns in credit card usage, repayment behavior, and factors influencing default occurrences among different demographic groups.\nSource: Default of credit card clients - UCI Machine Learning Repository. (n.d.). https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients\n\nStructure & Description\n\n\n\n\n\n\n\n\nColumn Names\nType\nDescription\nModified Column Names\n\n\n\n\nID\nCategorical/Numerical\n(Identifier)\nUnique identifier for each credit card holder.\nID\n\n\nLIMIT_BAL\nNumerical\n(Continuous)\nCredit limit assigned to each credit card holder.\nLimited_Bal\n\n\nSEX\nCategorical\n(Male/Female)\nGender of each credit card holder.\nGender\n\n\nEDUCATION\nCategorical\n(University/Grad/High_school/\nOthers/PG/Advanced_degree)\nEducational level of each credit card holder.\nEducation\n\n\nMARRIAGE\nCategorical\n(Married/Single/Others)\nMarital status of each credit card holder.\nMarriage\n\n\nAGE\nNumerical\n(Continuous)\nAge of each credit card holder.\nAge\n\n\nPAY_0\nCategorical\n(Ordinal)\nRepayment status in September\nRepay_Sept\n\n\nPAY_2\nCategorical\n(Ordinal)\nRepayment status in August\nRepay_Aug\n\n\nPAY_3\nCategorical\n(Ordinal)\nRepayment status in July\nRepay_Jul\n\n\nPAY_4\nCategorical\n(Ordinal)\nRepayment status in June\nRepay_Jun\n\n\nPAY_5\nCategorical\n(Ordinal)\nRepayment status in May\nRepay_May\n\n\nPAY_6\nCategorical\n(Ordinal)\nRepayment status in April\nRepay_Apr\n\n\nBILL_AMT1\nNumerical\n(Continuous)\nBill statement in September\nBillpay_Sept\n\n\nBILL_AMT2\nNumerical\n(Continuous)\nBill statement in August\nBillpay_Aug\n\n\nBILL_AMT3\nNumerical\n(Continuous)\nBill statement in July\nBillpay_Jul\n\n\nBILL_AMT4\nNumerical\n(Continuous)\nBill statement in June\nBillpay_Jun\n\n\nBILL_AMT5\nNumerical\n(Continuous)\nBill statement in May\nBillpay_May\n\n\nBILL_AMT6\nNumerical\n(Continuous)\nBill statement in April\nBillpay_Apr\n\n\nPAY_AMT1\nNumerical\n(Continuous)\nAmount paid in September\nAmtpaid_Sept\n\n\nPAY_AMT2\nNumerical\n(Continuous)\nAmount paid in August\nAmtpaid_Aug\n\n\nPAY_AMT3\nNumerical\n(Continuous)\nAmount paid in July\nAmtpaid_Jul\n\n\nPAY_AMT4\nNumerical\n(Continuous)\nAmount paid in June\nAmtpaid_Jun\n\n\nPAY_AMT5\nNumerical\n(Continuous)\nAmount paid in May\nAmtpaid_May\n\n\nPAY_AMT6\nNumerical\n(Continuous)\nAmount paid in April\nAmtpaid_Apr\n\n\ndefault payment next month\nCategorical\n(Default/Not Default)\nIndicates whether the individual defaulted on their payment in the following month.\ndefault_payment_next_month\n\n\n\n\nData Analysis:\n\n\n\n\n\n\nThe above graph showcases a structured presentation of demographic data, specifically focusing on the intersection of age and educational attainment. The “Education” column categorizes individuals into groups like “Advanced_degree”, “Grad”, “High_school”, “PG (Post Graduation)”, “Others”. X-axis represents the age and Y-axis represents the credit limit of the individuals. The information can potentially be utilized to understand the educational demographics within certain age groups and to inform their respective credit limits.\nThe graph provides an interpretation of a scatter plot with smooth trend lines depicting the relationship between age and credit limit across various educational backgrounds. It breaks down the trends observed in the data for each educational category and age group, highlighting how credit limits change with age within each educational group. The interpretation considers factors such as career trajectories, earning potential associated with different levels of education, and the variance in credit limits within each category. It also discusses the statistical methods used, such as smooth lines, to reveal overall trends while minimizing the influence of individual data points. Overall, the visualization offers valuable insights into the impact of educational background on financial credibility, aiding both financial institutions in assessing credit risk and individuals in understanding their financial standing relative to their education level.\n\n\n\n\n\n\nThe histogram of age distribution in the dataset reveals several important insights. Firstly, it highlights a concentration of individuals in their late 20s to early 40s, likely representing those in the midst of their careers. Secondly, it shows a right-skewed distribution, indicating a predominance of younger individuals, which could suggest a younger population or a higher propensity for credit among the younger demographic. Thirdly, the histogram’s use of 5-year bins effectively visualizes age distribution without oversimplification, with the highest frequency bars clustered around 25 to 35 years. Fourthly, the implications for financial products are significant, as institutions may tailor offerings and marketing strategies to cater to the economically active age group. Lastly, lower frequencies among older age groups prompt considerations for how financial services are tailored, particularly in regions with varying age demographics. Overall, the distribution underscores the importance of understanding age dynamics in shaping credit risk assessment, product design, and marketing strategies within the financial sector.\n\n\n\n\n\n\nThe bar chart illustrates the gender distribution within the dataset, revealing a higher representation of females compared to males. This suggests potential gender-based differences in credit application or dataset composition. The visualization’s simplicity aids in comparing gender counts effectively. Financial institutions can leverage this insight to tailor services and marketing strategies to better suit the demographic composition.\n\n\n\n\n\nThe bar chart depicts the distribution of default payment status for the next month, highlighting a majority of individuals expected not to default. This insight is vital for risk management, allowing financial institutions to tailor strategies to mitigate default risk effectively. The visualization’s clarity, with color-coded bars and direct labeling, facilitates easy comprehension of default status distribution. It informs business strategies such as provisioning for bad debts and designing credit products, ultimately aiding in strategic planning and operational adjustments in financial services."
  },
  {
    "objectID": "Graph 1.html",
    "href": "Graph 1.html",
    "title": "Code & Implementation",
    "section": "",
    "text": "Canadian National Broadcast Company showing their sources of funding in two financial years.\n\n\n\nRedesign 1\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(lattice)\n\n\nSources_of_Funding &lt;- c('Government_Funding', 'Revenue', 'Financing_and_Other_Income',\n       'Subscriber_Fees', 'Advertising', 'Digital', 'Television')\n\nYear1_Funding &lt;- c(1207.7,573.1,126.6,127.2,318.3,42.6,275.7)\nYear2_Funding &lt;- c(1213.7,490.1,116.9,124.4,248.8,31.0,217.8)\n\nm &lt;- list(\n  l = 50,\n  r = 50,\n  b = 50,\n  t = 70\n)\n\n\nFunding &lt;- data.frame(Sources_of_Funding, Year1_Funding, Year2_Funding)\nFunding\n\n          Sources_of_Funding Year1_Funding Year2_Funding\n1         Government_Funding        1207.7        1213.7\n2                    Revenue         573.1         490.1\n3 Financing_and_Other_Income         126.6         116.9\n4            Subscriber_Fees         127.2         124.4\n5                Advertising         318.3         248.8\n6                    Digital          42.6          31.0\n7                 Television         275.7         217.8\n\n\n\nPlot1 = plot_ly(Funding, y = ~Sources_of_Funding, x = ~Year1_Funding, type = 'bar', name = '2017-2018',marker=list(color = 'red')) %&gt;% \n    add_trace(x = ~Year2_Funding, name = '2018-2019', \n              marker = list(color = 'blue')) %&gt;%\n    layout(title= list(text=\"&lt;b&gt; Canadian National Broadcast Company showing\\ntheir sources of funding in two financial years. &lt;/b&gt;\",x=0.5,y=0.9,xanchor='center',yanchor='top'),font=list(size=10))  %&gt;%\n    layout(legend= list(title=list(text='&lt;i&gt;&lt;b&gt;Year&lt;/b&gt;&lt;/i&gt;'),x=100,y=0.5),\n           xaxis = list(title = \"&lt;b&gt;Funds in Millions&lt;b&gt;\",\n                   tick0=0, dtick=250,font=list(size=8.5)),\n           yaxis = list(title = \"&lt;b&gt;Source of Funding&lt;/b&gt;\",font=list(size=8.5)),\n           margin = m,\n           barmode='group')\n\nPlot1\n\n\n\n\n\n\n\nRedesign 2\n\nYear                       &lt;- c(\"2017-2018\", \"2018-2019\")\nGovernment_Funding         &lt;- c(1207.7, 1213.7)\nRevenue                    &lt;- c(573.1, 490.1)\nFinancing_and_Other_Income &lt;- c(126.6, 116.9)\nSubscriber_Fees            &lt;- c(127.2, 124.4)\nAdvertising                &lt;- c(318.3, 248.8)\nDigital                    &lt;- c(42.6, 31.0)\nTelevision                 &lt;- c(275.7, 217.8)\n\n\nmat &lt;- data.frame(Year, Government_Funding,Revenue,Financing_and_Other_Income,Subscriber_Fees,Advertising,Digital,Television)\nmat \n\n       Year Government_Funding Revenue Financing_and_Other_Income\n1 2017-2018             1207.7   573.1                      126.6\n2 2018-2019             1213.7   490.1                      116.9\n  Subscriber_Fees Advertising Digital Television\n1           127.2       318.3    42.6      275.7\n2           124.4       248.8    31.0      217.8\n\n\n\nFunds = gather(mat,key = Source_of_Funding,value = Funding,Government_Funding:Television,factor_key = T)\nFunds\n\n        Year          Source_of_Funding Funding\n1  2017-2018         Government_Funding  1207.7\n2  2018-2019         Government_Funding  1213.7\n3  2017-2018                    Revenue   573.1\n4  2018-2019                    Revenue   490.1\n5  2017-2018 Financing_and_Other_Income   126.6\n6  2018-2019 Financing_and_Other_Income   116.9\n7  2017-2018            Subscriber_Fees   127.2\n8  2018-2019            Subscriber_Fees   124.4\n9  2017-2018                Advertising   318.3\n10 2018-2019                Advertising   248.8\n11 2017-2018                    Digital    42.6\n12 2018-2019                    Digital    31.0\n13 2017-2018                 Television   275.7\n14 2018-2019                 Television   217.8\n\n\n\nPlot2 = ggplot(Funds, aes(x= Funding,y=Year,fill=Source_of_Funding,group = Source_of_Funding)) +\n  geom_point(shape = 21, size = 1.5) + \n  geom_line(aes(color = Source_of_Funding)) +\n  labs(title=\"Canadian National Broadcast Company showing their sources of\\nfunding in two financial years.\",fill=\"Source of Funding\",x=\"Funds in Millions\",y=\"Year\")+\n  scale_x_continuous(breaks=seq(0,1250,by=150)) + \n  theme(legend.position =\"none\",\n        plot.margin = margin(t = 25,  \n                             r = 5,  \n                             b = 5,  \n                             l = 6),\n        plot.title=element_text(size=10,face=\"bold\",hjust=0.5),\n        axis.text=element_text(size=6),\n        axis.title=element_text(size=8.5,face=\"bold\"))+\n  facet_wrap(~Source_of_Funding)\n\nPlot2\n\n\n\n\nUsing ggplotly to make the above graph interactive.\n\nggplotly(Plot2)"
  },
  {
    "objectID": "Des1.html",
    "href": "Des1.html",
    "title": "Graph - Redesigns",
    "section": "",
    "text": "Let us observe into the above chart from the Canadian National Broadcast Company showing their sources of funding in two financial years. It looks simple and easy to understand, but there are few flaws in it. Let’s have a look into it.\n\nWe can see that the Y-axis scale is not properly arranged. It has a break in it. The lower ticks on the Y-axis scale are separated at $100M. But there is a sudden increase on the scale from $700M to $1700M. Due to this, we can see that the bar of the Revenue in both the years (2017-2018 and 2018-2019) is greater than the Government Funding which is not true.\nThe labels for the Revenue and Advertising Revenue are placed on the wrong bars, which is leading to the misinterpretation of the graph.\nAnother issue we found is that the Television is shown equal to that of the Government Funding because of the break on the Y-axis scale. We can say that the above graph is a misleading graph which is not providing us with the proper data visualization.\n\nBelow are the two redesigns which will help us to understand the given data in a better way.\n\nRedesign 1\nThe first redesign is a bar plot plotted between the Funds (in Millions) and the Source of Funding for both the given years.\n\n\n\n\n\n\n\n\nRedesign 2\nThe second redesign is a Juxtaposed graph. In this graph we can see the seven Sources of Funding shown for the given two years."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Introduction",
    "section": "",
    "text": "For our final project in “Applied Statistics and Visualization for Analytics,” we conducted a “Credit Card Default Analysis.” Our team, consisting of three members—Pravalika, Sowdhamini, and myself—aimed to analyze a dataset titled “Default of Credit Card Clients” which will provide insights into the patterns and factors influencing credit card default occurrences, aiding in risk assessment and decision-making in the financial domain.\nThe primary objectives of the analysis are:\n\nCertain demographic groups exhibit a higher or lower likelihood of defaulting on credit card payments\nThe most important features for predicting credit default risk\nDistinct patterns of credit limit utilization among users."
  },
  {
    "objectID": "Codes.html",
    "href": "Codes.html",
    "title": "Codes",
    "section": "",
    "text": "Research Question 1:\nDo certain demographic groups have a higher or lower likelihood of defaulting on credit card payments, and can this be predicted using regression models?\nCode:\n\n# Load necessary libraries\nlibrary(readr)  # For data reading\nlibrary(dplyr)  # For data manipulation\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)  # For data visualization\nlibrary(caret)  # For machine learning models\n\nLoading required package: lattice\n\n# Read the dataset\ncredit_data &lt;- read.csv(\"C:\\\\Users\\\\nmadh\\\\OneDrive\\\\Desktop\\\\Final Project\\\\Final_output.csv\")\n\n# 1. Preprocess the data\n# Convert categorical variables to factors\ncredit_data$Gender &lt;- as.factor(credit_data$Gender)\ncredit_data$Education &lt;- as.factor(credit_data$Education)\ncredit_data$Marriage &lt;- as.factor(credit_data$Marriage)\ncredit_data$default_payment_next_month &lt;- as.factor(credit_data$default_payment_next_month)\n\n# Check for missing values\nsum(is.na(credit_data))\n\n[1] 0\n\n# 2. Explore the data\n# Visualize default payments across different demographic groups\nplot1 = ggplot(credit_data, aes(x = default_payment_next_month, fill = Gender)) +\n  geom_bar() +\n  facet_wrap(~ Education + Marriage, scales = \"free\") +\n  labs(title = \"Default Payments by Gender, Education, and Marriage Status\")\nplot1\n\n\n\n# 3. Train regression models\n# Split the data into training and testing sets\nset.seed(123)  # For reproducibility\ntrain_index &lt;- createDataPartition(credit_data$default_payment_next_month, p = 0.7, list = FALSE)\ntrain_data &lt;- credit_data[train_index, ]\ntest_data &lt;- credit_data[-train_index, ]\n\n# Train logistic regression model\nlogit_model &lt;- glm(default_payment_next_month ~ ., data = train_data, family = binomial)\n\n# 4. Evaluate model performance\n# Make predictions on the test set\npredictions &lt;- predict(logit_model, newdata = test_data, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, \"Default\", \"Not Default\")\n\n# Confusion matrix\nconfusionMatrix(table(predicted_classes, test_data$default_payment_next_month))\n\nConfusion Matrix and Statistics\n\n                 \npredicted_classes Default Not Default\n      Default        1508        6795\n      Not Default     482         214\n                                          \n               Accuracy : 0.1914          \n                 95% CI : (0.1833, 0.1996)\n    No Information Rate : 0.7789          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : -0.0991         \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.75779         \n            Specificity : 0.03053         \n         Pos Pred Value : 0.18162         \n         Neg Pred Value : 0.30747         \n             Prevalence : 0.22114         \n         Detection Rate : 0.16757         \n   Detection Prevalence : 0.92266         \n      Balanced Accuracy : 0.39416         \n                                          \n       'Positive' Class : Default         \n                                          \n\n\n\n\nResearch Question 2:\nWhich features are deemed most important in predicting credit default risk?\nCode:\n\ndata &lt;- read.csv(\"C:\\\\Users\\\\nmadh\\\\OneDrive\\\\Desktop\\\\Final Project\\\\Final_output.csv\")\n\ndata$Gender&lt;- factor(data$Gender)\ndata$Education &lt;- factor(data$Education)\ndata$Marriage &lt;- factor(data$Marriage)\ndata$Repay_Sept &lt;- factor(data$Repay_Sept)\ndata$Repay_Aug &lt;- factor(data$Repay_Aug)\ndata$Repay_Jul &lt;- factor(data$Repay_Jul)\ndata$Repay_Jun &lt;- factor(data$Repay_Jun)\ndata$Repay_May &lt;- factor(data$Repay_May)\ndata$Repay_Apr &lt;- factor(data$Repay_Apr)\ndata$default_payment_next_month &lt;- factor(data$default_payment_next_month)\n\n# Load required libraries\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Set seed for reproducibility\nset.seed(1234)\n\n# Train the Random Forest model\nrf_model &lt;- randomForest(default_payment_next_month ~ ., data = data)\n\n# Get variable importance measures\nimportance &lt;- importance(rf_model)\n\n# Sort variable importance measures by importance\nvar_importance &lt;- importance[order(-importance[, 1]), ]\n\n# Display variable importance measures\nprint(var_importance)\n\n  Repay_Sept           ID Billpay_Sept          Age  Billpay_Aug Amtpaid_Sept \n  1018.32726    672.52568    563.75433    534.11658    512.46432    493.17382 \n Billpay_Jul  Limited_Bal  Billpay_Jun  Billpay_Apr  Billpay_May  Amtpaid_Aug \n   491.20440    490.99607    478.71140    476.84633    469.35011    455.75366 \n Amtpaid_Jul  Amtpaid_Apr  Amtpaid_May  Amtpaid_Jun    Repay_Aug    Repay_Jul \n   436.19727    433.73568    417.53178    413.28457    410.82039    291.82828 \n   Repay_Jun    Repay_May    Repay_Apr    Education     Marriage       Gender \n   234.61829    214.68302    210.96277    184.38949    109.52670     88.19018 \n\n\nThe output shows the importance of different variables in predicting credit default risk. Repayment behavior in previous months, represented by variables like Repay_Sept, has the highest importance, indicating its strong influence on predicting default. Billing and payment amounts also contribute to prediction but to a lesser extent. Demographic variables like Age have some importance, while Customer ID has limited predictive power. Overall, understanding these factors helps in assessing credit risk and making informed decisions.\n\nstr(importance)\n\n num [1:24, 1] 672.5 491 88.2 184.4 109.5 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:24] \"ID\" \"Limited_Bal\" \"Gender\" \"Education\" ...\n  ..$ : chr \"MeanDecreaseGini\"\n\n\n\n# Extract the values of MeanDecreaseGini\nimportance_values &lt;- as.numeric(importance[, 1])\n\n# Sort feature importance values in descending order\nimportance_values_sorted &lt;- sort(importance_values, decreasing = TRUE)\n\n# Get corresponding feature names\nfeature_names &lt;- rownames(importance)[order(importance_values, decreasing = TRUE)]\n\n# Plot feature importance\nbarplot(importance_values_sorted, \n        main = \"Feature Importance in Predicting Credit Default Risk\",\n        xlab = \"Features\",\n        ylab = \"Mean Decrease in Gini Index\",\n        cex.names = 0.8,\n        las = 2,  # Rotate labels vertically\n        col = \"skyblue\",\n        ylim = c(0, max(importance_values_sorted) * 1.1),\n        names.arg = feature_names,\n        args.legend = list(x = \"topright\"),  # Adjust x-axis label position\n        mar = c(10, 10, 14, 12))  # Adjust margin (bottom margin increased)\n\n\n\n\n\n\nResearch Question 3:\nIdentify the distinct patterns of credit limit utilization among users.\nCode:\n\n# Load necessary libraries\nlibrary(cluster)\n\n# Load the dataset\ncredit_data &lt;- read.csv(\"C:\\\\Users\\\\nmadh\\\\OneDrive\\\\Desktop\\\\Final Project\\\\Final_output.csv\")\n\n# Step 2: Select relevant columns for clustering\ndata_for_clustering &lt;- credit_data[, c(\"Limited_Bal\", \"Repay_Apr\", \"Repay_May\", \"Repay_Jun\", \"Repay_Jul\", \"Repay_Aug\", \"Repay_Sept\")]\n\n# Step 3: Standardize the data\nscaled_data &lt;- scale(data_for_clustering)\n\n# Step 4: Determine optimal number of clusters using elbow method\nwss &lt;- (nrow(scaled_data)-1) * sum(apply(scaled_data, 2, var))\nfor (i in 2:15) wss[i] &lt;- sum(kmeans(scaled_data, centers=i)$withinss)\nplot(1:15, wss, type=\"b\", xlab=\"Number of Clusters\", ylab=\"Within groups sum of squares\")\n\n\n\n# Step 5: Based on the elbow plot, select the number of clusters (e.g., 3)\nnum_clusters &lt;- 3\n\n# Step 6: Perform K-means clustering\nkmeans_result &lt;- kmeans(scaled_data, centers=num_clusters)\n\n# Step 7: Assign cluster labels to each user\ncluster_labels &lt;- kmeans_result$cluster\n\n# Step 8: Add cluster labels to the original dataset\ncredit_data_with_clusters &lt;- cbind(credit_data, cluster = cluster_labels)\n\n# Step 9: Get the cluster centers\ncluster_centers &lt;- kmeans_result$centers\n\n# Step 10: Calculate standard deviations across columns\ncol_std_dev &lt;- apply(data_for_clustering, 2, sd)\n\n# Step 11: Scale the cluster centers by dividing by standard deviations\ncluster_centers &lt;- scale(cluster_centers, center = FALSE, scale = col_std_dev)\n\n# Step 12: Assign column names to cluster_centers\ncolnames(cluster_centers) &lt;- colnames(data_for_clustering)\n\n# Step 13: View cluster centers\nprint(cluster_centers)\n\n    Limited_Bal  Repay_Apr  Repay_May  Repay_Jun  Repay_Jul  Repay_Aug\n1 -4.699941e-06  1.2499464  1.4093804  1.4227930  1.4360907  1.4124949\n2  3.644460e-06 -0.8264215 -0.8801255 -0.8758454 -0.8553181 -0.8132726\n3 -9.206548e-07  0.1728956  0.1688987  0.1641300  0.1511985  0.1346120\n   Repay_Sept\n1  1.28978602\n2 -0.66272483\n3  0.08266683\nattr(,\"scaled:scale\")\n Limited_Bal    Repay_Apr    Repay_May    Repay_Jun    Repay_Jul    Repay_Aug \n1.297477e+05 1.149988e+00 1.133187e+00 1.169139e+00 1.196868e+00 1.197186e+00 \n  Repay_Sept \n1.123802e+00 \n\n# Visualization (Optional):\n# Plot clusters based on the first two principal components\npca_data &lt;- prcomp(scaled_data, center = TRUE, scale. = TRUE)\nplot(pca_data$x[, 1], pca_data$x[, 2], col = cluster_labels, pch = 20, main = \"Clusters based on Credit Utilization Patterns\", xlab = \"Principal Component 1\", ylab = \"Principal Component 2\")\nlegend(\"topright\", legend = unique(cluster_labels), col = unique(cluster_labels), pch = 20, title = \"Cluster\")"
  },
  {
    "objectID": "Des2.html",
    "href": "Des2.html",
    "title": "Graph - Redesigns",
    "section": "",
    "text": "Let us proceed with the second graph. The above bubble chart is trying to give us information regarding the Top 10 Films by Worldwide Grosses. It shows the names of the Films, Year - the film has released and its Production Budget. But there are a few issues with the above chart.\n\nUsually bubble charts are a way of evaluating the relationship between data points visually. In the above chart, there is no relationship found between the bubbles. If the bubbles were not labelled, drawing conclusions by the size of the bubbles might be difficult.\nEven though the Star Wars: Episode I-The Phantom Menace film’s production budget is equal to the production budget of the film Shrek 2, the placement of the bubbles in the above chart is uneven.\n\nBut the graph can be redesigned in a proper way for a better understanding and the best way of visual representation is a bar plot.\n\nRedesign 1\nIn the first redesign, we are plotting a bar graph using ggplotly to perform interactive data visualization.\n\n\n\n\n\n\n\n\nRedesign 2\nThe below graph is the second redesign for the above bubble chart. We are plotting an interactive scatter plot using ggplotly where each point in the graph represents a Film."
  },
  {
    "objectID": "Graph 2.html",
    "href": "Graph 2.html",
    "title": "Code & Implementation",
    "section": "",
    "text": "Top 10 Films by Worldwide Grosses\n\n\nData analysed from the graph\n\nYear_of_Release &lt;- c(\"1997\",\"1999\",\"2001\",\"2002\",\"2003\",\"2004\",\"2006\",\"2007\",\"2007\",\"2008\")\n\nFilm &lt;- c(\"Titanic\",\n          \"Star Wars:Episode I-The Phantom Menace\",\n          \"Harry Potter and the Sorcerer's Stone\",\n          \"The Lord of Rings:The Two Towers\",\n          \"The Lord of Rings:The Return of the King\",\n          \"Shrek 2\",\n          \"Pirates of the Carribean:Dead Man's Chest\",\n          \"Pirates of the Carribean:At World's End\",\n          \"Harry Potter and the Order of the Phoenix\",\n          \"The Dark Knight\")\n\nProduction_Budget &lt;- c(1.84,0.92,0.98,0.93,1.12,0.92,1.07,0.96,0.94,0.95)\n\n\nfilmdata &lt;- data.frame(Year_of_Release,Film,\n                       Production_Budget)\nfilmdata\n\n   Year_of_Release                                      Film Production_Budget\n1             1997                                   Titanic              1.84\n2             1999    Star Wars:Episode I-The Phantom Menace              0.92\n3             2001     Harry Potter and the Sorcerer's Stone              0.98\n4             2002          The Lord of Rings:The Two Towers              0.93\n5             2003  The Lord of Rings:The Return of the King              1.12\n6             2004                                   Shrek 2              0.92\n7             2006 Pirates of the Carribean:Dead Man's Chest              1.07\n8             2007   Pirates of the Carribean:At World's End              0.96\n9             2007 Harry Potter and the Order of the Phoenix              0.94\n10            2008                           The Dark Knight              0.95\n\n\nLibraries\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nRedesign 1\n\nPlot3 = ggplot(filmdata, aes( x=Year_of_Release,y=Production_Budget))+\n  geom_bar(stat = \"identity\", color=\"black\",aes(fill = Film),position = \"dodge\")+\n  scale_y_continuous(limits=c(0,2))+\n  labs(x =\"Release Year\",y=\"Production Budget in Billions\",title =\"Top 10 Films by Worldwide Grosses\")+\n  scale_fill_manual(values = c(rgb(0,0.3,0),\"grey\",\"yellow\",\"skyblue\",\"orange\",\"blue\",\"purple\",\"green\",\"pink\",\"red\"))+\n  theme(legend.text=element_text(size=8,face=\"italic\"),\n        legend.title=element_text(size=8.5,face=\"bold\"),\n        plot.title=element_text(size=10,face=\"bold\",hjust=0.5),\n        axis.title=element_text(size=8.5,face=\"bold\"))\n\nPlot3\n\n\n\n\nUsing ggplotly to make the above graph interactive.\n\nggplotly(Plot3)\n\n\n\n\n\n\n\nRedesign 2\n\nPlot4 = ggplot(filmdata, aes(x = Year_of_Release, y = Production_Budget)) + \n  geom_point(aes(color = Film))+\n  scale_y_continuous(limits = c(0,2), breaks = seq(0,2,.1))+\n  labs(x =\"Release Year\",y=\"Production Budget in Billions\",title =\"Top 10 Films by Worldwide Grosses\")+\n  scale_color_manual(values = c(rgb(0,0.5,0),\"maroon\",\"yellow\",\"skyblue\",\"orange\",\"blue\",\"purple\",\"green\",\"pink\",\"red\"))+\n  theme(legend.text=element_text(size=8,face=\"italic\"),\n        legend.title=element_text(size=8.5,face=\"bold\"),\n        plot.title=element_text(size=10,face=\"bold\",hjust=0.5),\n        axis.title=element_text(size=8.5,face=\"bold\"))\n\nPlot4\n\n\n\n\nUsing ggplotly to make the above graph interactive.\n\nggplotly(Plot4)"
  },
  {
    "objectID": "index2.html",
    "href": "index2.html",
    "title": "Introduction",
    "section": "",
    "text": "For our mid-term project in “Applied Statistics and Visualization for Analytics,” Sowdhamini and I worked as a team. This project focused on redesigning two poorly constructed graphs to ensure accurate data representation. We prioritized key elements such as clear data presentation, simple design, consistent scaling, and appropriate labels and titles. Embedded code for each redesign is provided for reference.\n\nThe first poorly constructed graph, originally a stacked bar chart, has been redesigned into a simplified bar chart and a juxtaposed chart.\nThe second poorly constructed graph, initially a bubble chart, has been redesigned into a simplified bar chart and a scatter plot.\n\n\n\n\n\n\nWorking on this project was an exciting journey for us as we got an opportunity to explore and gain hands-on learning on the topics we learnt so far.\nFeel free to watch the YouTube link: STAT Mid Project\nMadhu Yamini Nainala"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Default of credit card clients - UCI Machine Learning Repository. (n.d.).\nPrabhakaran, S. (n.d.). Logistic regression with R."
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "ProEd.com",
    "section": "",
    "text": "I’ve created this website to store and showcase my projects. Feel free to explore and provide feedback."
  }
]